{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: proc_ont.py\n",
    "# Description:\n",
    "# This file accepts user made strings as input.\n",
    "# It processes the input to clean and extract keywords from it.\n",
    "# Then the tool proceeds with extracting the ontology concept for the keyword.\n",
    "# Afterwards, the concepts get queried to a search tool (default is GeoNetwork).\n",
    "# The output is converted to human readable format.\n",
    "# Author: Mitchell Verhaar\n",
    "\n",
    "from owlready2 import *\n",
    "from nltk.corpus import stopwords\n",
    "from timeit import default_timer as timer\n",
    "import string\n",
    "import re\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ontology_Parser:\n",
    "\n",
    "    def __init__(self, ont_list, filt_punc = False):\n",
    "        ### Initializes the class instance\n",
    "        self.ont_list = ont_list\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.punc_table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        self.filter_punc = filt_punc\n",
    "        self.ont_dict_split = {}\n",
    "        self.ont_dict_count = {}\n",
    "        self.url = \"http://212.189.145.37:8080/geonetwork/srv/eng/q?or=%s&from=1&resultType=details&fast=index&_content_type=xml\"\n",
    "        self.u_input = []\n",
    "        self.processed_query = ''\n",
    "\n",
    "    def load_ontologies(self):\n",
    "        ### This function loads the ontologies given to the class at the initialization\n",
    "        for ont in self.ont_list:\n",
    "            try:\n",
    "                self.ont_dict_split[World().get_ontology(ont).load()] = {}\n",
    "            except Exception as err:\n",
    "                print(\"Cannot load ontology: \" + ont + \"\\nDue to error: \" + str(err))\n",
    "    \n",
    "    def process_input(self, u_input = None):\n",
    "        ### This function cleans and filters the user input into usable input terms\n",
    "        if u_input and self.filter_punc:\n",
    "            return [re.sub(r\"\\W+$\", \"\", re.sub(r\"\\W+\\b\", \" \", word.lower())) for word in u_input.split(',') if word.lower() not in self.stopwords]\n",
    "        elif u_input and not self.filter_punc:  \n",
    "            return [word.lower() for word in u_input.split(',') if word.lower() not in self.stopwords]\n",
    "        elif not u_input and self.filter_punc:\n",
    "            return [re.sub(r\"\\W+$\", \"\", re.sub(r\"\\W+\\b\", \" \", word.lower())) for word in input(\"Enter the desired query here: \").split(',') if word.lower() not in self.stopwords]\n",
    "        else:\n",
    "            return [word.lower() for word in input(\"Enter the desired query here: \").split(',') if word.lower() not in self.stopwords]\n",
    "        \n",
    "    def search(self, u_input = None):\n",
    "        ### This function performs the searching of ontologies given a set of input terms\n",
    "        self.clean_data()\n",
    "        self.u_input = self.process_input(u_input)\n",
    "        if '' not in self.u_input:\n",
    "            for ont in self.ont_dict_split.keys():\n",
    "                for term in self.u_input:\n",
    "                    for ind in ont.individuals():\n",
    "                        if hasattr(ind, 'label'):\n",
    "                            if term in (concept.lower() for concept in ind.label):\n",
    "                                print('passed with term: ' + term)\n",
    "                                self.add_concepts(ind, ont)\n",
    "                    ont_concept = ont.search_one(label = term)\n",
    "                    if ont_concept:\n",
    "                        self.add_concepts(ont_concept, ont)\n",
    "                    else:\n",
    "                        if term.lower() not in (keyword.lower() for keyword in self.ont_dict_count.keys()):\n",
    "                            self.add_count(term.lower())\n",
    "            \n",
    "        else:\n",
    "            print(\"Input must be provided!\")\n",
    "    \n",
    "    def add_concepts(self, result, ont):\n",
    "        ### Adds concepts to the original query keywords\n",
    "        self.ont_dict_split[ont][result.label[0]] = result\n",
    "        self.add_count(result.label[0])\n",
    "        for related_concepts in result.is_a:\n",
    "            if hasattr(related_concepts, 'label'):\n",
    "                    if related_concepts.label and 'Obsolete Class' not in related_concepts.label:\n",
    "                        self.ont_dict_split[ont][related_concepts.label[0]] = related_concepts\n",
    "                        self.add_count(related_concepts.label[0].lower())\n",
    "            else:\n",
    "                if hasattr(related_concepts, 'value'):\n",
    "                    try:\n",
    "                        self.ont_dict_split[ont][related_concepts.value.label[0]] = related_concepts.value\n",
    "                        self.add_count(related_concepts.value.label[0].lower())\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "    def process_concepts(self):\n",
    "        ### This function converts the ontology output into a list sorted by most occurring concept\n",
    "        sorted_concepts = sorted(self.ont_dict_count.items(), key=lambda item: item[1])\n",
    "        return [concept[0] for concept in sorted_concepts]\n",
    "    \n",
    "    def add_count(self, label):\n",
    "        ### Adds term to stored dict\n",
    "        if label in self.ont_dict_count:\n",
    "            self.ont_dict_count[label] += 1\n",
    "        else:\n",
    "            self.ont_dict_count[label] = 1\n",
    "            \n",
    "    def send_query(self, query):\n",
    "        ### This function sends the given query to an external search tool, GeoNetwork in this case.\n",
    "        try:\n",
    "            response = requests.get(self.url % query)\n",
    "            with open(\"Search_Results/output_\" + query.replace(\" \", \"_\") + \".xml\", \"w+\") as f:\n",
    "                f.write(response.text)\n",
    "                print(\"Output saved to Search_Results/output_\" + query.replace(\" \", \"_\"))\n",
    "        except:\n",
    "            print(\"Connection to the search tool failed!\")\n",
    "\n",
    "    def process_query(self, keywords = ''):\n",
    "        ### This function sends the ontology output to the search tool that is being used\n",
    "        self.processed_query = ' '.join(self.process_concepts())\n",
    "        if keywords:\n",
    "            print(\"Given input query: \" + keywords)\n",
    "            self.send_query(keywords)\n",
    "        elif self.processed_query:\n",
    "            print(\"Ontological supported query: \" + self.processed_query)\n",
    "            self.send_query(self.processed_query)\n",
    "        \n",
    "        else:\n",
    "            if self.u_input:\n",
    "                u_input = ' '.join(self.u_input)\n",
    "                print(\"No ontology concepts could be found, using original user query as input: \" + u_input)\n",
    "                self.send_query(u_input)\n",
    "            else:\n",
    "                print('No query has been given, aborting search!')\n",
    "    \n",
    "    def clean_data(self):\n",
    "        ### Cleans the keywords gathered from previous searches\n",
    "        self.ont_dict_count = {}\n",
    "        for ont in self.ont_dict_split:\n",
    "            self.ont_dict_split[ont] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class initialiation and ontology loading (excessive runtime warning!)\n",
    "\n",
    "### Initializes the Class with a list of ontology's to be loaded in and a flag to process punctuation\n",
    "ont_parse = Ontology_Parser([\"http://purl.obolibrary.org/obo/envo.owl\", \"http://purl.obolibrary.org/obo/geo.owl\"], False)\n",
    "\n",
    "### Loads the ontology's into the class variables\n",
    "ont_parse.load_ontologies()\n",
    "\n",
    "### Searches the ontology's and retrieves all concepts that relate to the input\n",
    "#ont_parse.search()\n",
    "\n",
    "### Processes input into query to send to GeoNetwork\n",
    "#ont_parse.process_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timed_q = {}\n",
    "\n",
    "def time_search(q, process = False):\n",
    "    ### This function measures the runtime of the total process\n",
    "    if process:\n",
    "        start = timer()\n",
    "        ont_parse.search(q)\n",
    "        ont_parse.process_query()\n",
    "        end = timer()\n",
    "        timed_q[ont_parse.processed_query] = round((end-start)*1000)\n",
    "    else:\n",
    "        start = timer()\n",
    "        ont_parse.process_query(q)\n",
    "        end = timer()\n",
    "        timed_q[q] = round((end-start)*1000)\n",
    "    print(\"Duration of search & send: \" + str((end - start)*1000) + ' ms\\n')\n",
    "    \n",
    "time_search('Ocean', False)\n",
    "time_search('ocean', True)\n",
    "time_search('Ocean,water body', False)\n",
    "time_search('Ocean,water body', True)\n",
    "time_search('Continent', False)\n",
    "time_search('Continent', True)\n",
    "time_search('Ocean,World Ocean,Marine water body,Cave', False)\n",
    "time_search('Ocean,World Ocean,Marine water body,Cave', True)\n",
    "time_search('Texel,Island,Sea', False)\n",
    "time_search('Texel,Island,Sea', True)\n",
    "time_search('Earth,Ocean', False)\n",
    "time_search('Earth,Ocean', True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shows the contents of the double dictionary that keeps track of the ontology's and the concepts found within the ontology's\n",
    "#print(ont_parse.ont_dict_split)\n",
    "\n",
    "### Shows the contents of the count dictionary, which counts the amount of times a concept has been found in all the ontology's\n",
    "#print(ont_parse.ont_dict_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell is responsible for creating the graph only. Delete this in case it's not needed! ###\n",
    "\n",
    "labels = [\"Set 1\", \"Set 2\", \"Set 3\", \"Set 4\", \"Set 5\", \"Set 6\"]\n",
    "\n",
    "query_1 = list(timed_q.values())[0::2]\n",
    "query_2 = list(timed_q.values())[1::2]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, query_1, width, label='Original Query')\n",
    "rects2 = ax.bar(x + width/2, query_2, width, label='Enhanced Query')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Runtime (ms)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.075),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('graph.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python37064bitbc436b90dab04c358fa0eaa6468e0142"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
